{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.optim.adamw import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "from wolf.data import load_datasets, get_batch, preprocess, postprocess\n",
    "from wolf import WolfModel\n",
    "from wolf.utils import total_grad_norm\n",
    "from wolf.optim import ExponentialScheduler\n",
    "\n",
    "from experiments.options import parse_args\n",
    "\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_master(rank):\n",
    "    return rank <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_distributed(rank):\n",
    "    return rank >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(info, logfile=None):\n",
    "    print(info)\n",
    "    if logfile is not None:\n",
    "        print(info, file=logfile)\n",
    "        logfile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(learning_rate, parameters, betas, eps, amsgrad, step_decay, weight_decay, warmup_steps, init_lr):\n",
    "    optimizer = AdamW(parameters, lr=learning_rate, betas=betas, eps=eps, amsgrad=amsgrad, weight_decay=weight_decay)\n",
    "    step_decay = step_decay\n",
    "    scheduler = ExponentialScheduler(optimizer, step_decay, warmup_steps, init_lr)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(args):\n",
    "    def check_dataset():\n",
    "        if dataset == 'cifar10':\n",
    "            assert image_size == 32, 'CIFAR-10 expected image size 32 but got {}'.format(image_size)\n",
    "        elif dataset.startswith('lsun'):\n",
    "            assert image_size in [128, 256]\n",
    "        elif dataset == 'celeba':\n",
    "            assert image_size in [256, 512]\n",
    "        elif dataset == 'imagenet':\n",
    "            assert image_size in [64, 128, 256]\n",
    "\n",
    "    dataset = args.dataset\n",
    "    if args.category is not None:\n",
    "        dataset = dataset + '_' + args.category\n",
    "    image_size = args.image_size\n",
    "    check_dataset()\n",
    "\n",
    "    nc = 3\n",
    "    args.nx = image_size ** 2 * nc\n",
    "    n_bits = args.n_bits\n",
    "    args.n_bins = 2. ** n_bits\n",
    "    args.test_k = 5\n",
    "\n",
    "    model_path = args.model_path\n",
    "    args.checkpoint_name = os.path.join(model_path, 'checkpoint')\n",
    "\n",
    "    result_path = os.path.join(model_path, 'images')\n",
    "    args.result_path = result_path\n",
    "    data_path = args.data_path\n",
    "\n",
    "    if is_master(args.rank):\n",
    "        if not os.path.exists(model_path):\n",
    "            os.makedirs(model_path)\n",
    "        if not os.path.exists(result_path):\n",
    "            os.makedirs(result_path)\n",
    "        if args.recover < 0:\n",
    "            args.log = open(os.path.join(model_path, 'log.txt'), 'w')\n",
    "        else:\n",
    "            args.log = open(os.path.join(model_path, 'log.txt'), 'a')\n",
    "    else:\n",
    "        args.log = None\n",
    "\n",
    "    args.cuda = torch.cuda.is_available()\n",
    "    random_seed = args.seed + args.rank if args.rank >= 0 else args.seed\n",
    "    if args.recover >= 0:\n",
    "        random_seed += random.randint(0, 1024)\n",
    "    logging(\"Rank {}: random seed={}\".format(args.rank, random_seed), logfile=args.log)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    device = torch.device('cuda', args.local_rank) if args.cuda else torch.device('cpu')\n",
    "    if args.cuda:\n",
    "        torch.cuda.set_device(device)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    args.world_size = int(os.environ[\"WORLD_SIZE\"]) if is_distributed(args.rank) else 1\n",
    "    logging(\"Rank {}: \".format(args.rank) + str(args), args.log)\n",
    "\n",
    "    train_data, val_data = load_datasets(dataset, image_size, data_path=data_path)\n",
    "    train_index = np.arange(len(train_data))\n",
    "    np.random.shuffle(train_index)\n",
    "    val_index = np.arange(len(val_data))\n",
    "\n",
    "    if is_master(args.rank):\n",
    "        logging('Data size: training: {}, val: {}'.format(len(train_index), len(val_index)))\n",
    "\n",
    "    if args.recover >= 0:\n",
    "        params = json.load(open(os.path.join(model_path, 'config.json'), 'r'))\n",
    "    else:\n",
    "        params = json.load(open(args.config, 'r'))\n",
    "        json.dump(params, open(os.path.join(model_path, 'config.json'), 'w'), indent=2)\n",
    "\n",
    "    wolf = WolfModel.from_params(params)\n",
    "    wolf.to_device(device)\n",
    "    args.device = device\n",
    "\n",
    "    return args, (train_data, val_data), (train_index, val_index), wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dataloader(args, train_data, val_data):\n",
    "    if is_distributed(args.rank):\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_data, rank=args.rank,\n",
    "                                                                        num_replicas=args.world_size,\n",
    "                                                                        shuffle=True)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "    train_loader = DataLoader(train_data, batch_size=args.batch_size,\n",
    "                              shuffle=(train_sampler is None), sampler=train_sampler,\n",
    "                              num_workers=args.workers, pin_memory=True, drop_last=True)\n",
    "    if is_master(args.rank):\n",
    "        eval_batch = args.eval_batch_size\n",
    "        val_loader = DataLoader(val_data, batch_size=eval_batch, shuffle=False,\n",
    "                                num_workers=args.workers, pin_memory=True)\n",
    "    else:\n",
    "        val_loader = None\n",
    "\n",
    "    return train_loader, train_sampler, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(args, train_data, train_index, wolf):\n",
    "    wolf.eval()\n",
    "    init_batch_size = args.init_batch_size\n",
    "    logging('Rank {}, init model: {} instances'.format(args.rank, init_batch_size), args.log)\n",
    "    init_index = np.random.choice(train_index, init_batch_size, replace=False)\n",
    "    init_x, init_y = get_batch(train_data, init_index)\n",
    "    init_x = preprocess(init_x.to(args.device), args.n_bits)\n",
    "    init_y = init_y.to(args.device)\n",
    "    print(init_x.shape)\n",
    "    print(init_y.shape)\n",
    "    wolf.init(init_x, y=init_y, init_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(args, epoch, val_data, val_index, wolf):\n",
    "    logging('reconstruct', args.log)\n",
    "    wolf.eval()\n",
    "    n = 16\n",
    "    np.random.shuffle(val_index)\n",
    "    img, y = get_batch(val_data, val_index[:n])\n",
    "    img = img.to(args.device)\n",
    "    y = y.to(args.device)\n",
    "\n",
    "    z, epsilon = wolf.encode(img, y=y, n_bits=args.n_bits, random=False)\n",
    "    epsilon = epsilon.squeeze(1)\n",
    "    z = z.squeeze(1) if z is not None else z\n",
    "    img_recon = wolf.decode(epsilon, z=z, n_bits=args.n_bits)\n",
    "\n",
    "    img = postprocess(preprocess(img, args.n_bits), args.n_bits)\n",
    "    abs_err = img_recon.add(img * -1).abs()\n",
    "    logging('Err: {:.4f}, {:.4f}'.format(abs_err.max().item(), abs_err.mean().item()), args.log)\n",
    "\n",
    "    comparison = torch.cat([img, img_recon], dim=0).cpu()\n",
    "    reorder_index = torch.from_numpy(np.array([[i + j * n for j in range(2)] for i in range(n)])).view(-1)\n",
    "    comparison = comparison[reorder_index]\n",
    "    image_file = 'reconstruct{}.png'.format(epoch)\n",
    "    save_image(comparison, os.path.join(args.result_path, image_file), nrow=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(args, epoch, wolf):\n",
    "    logging('sampling', args.log)\n",
    "    wolf.eval()\n",
    "    n = 64 if args.image_size > 128 else 256\n",
    "    nrow = int(math.sqrt(n))\n",
    "    taus = [0.7, 0.8, 0.9, 1.0]\n",
    "    start_time = time.time()\n",
    "    image_size = (3, args.image_size, args.image_size)\n",
    "    for t in taus:\n",
    "        imgs = wolf.synthesize(n, image_size, tau=t, n_bits=args.n_bits, device=args.device)\n",
    "        image_file = 'sample{}.t{:.1f}.png'.format(epoch, t)\n",
    "        save_image(imgs, os.path.join(args.result_path, image_file), nrow=nrow)\n",
    "    logging('time: {:.1f}s'.format(time.time() - start_time), args.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(args, val_loader, wolf):\n",
    "    wolf.eval()\n",
    "    wolf.sync()\n",
    "    gnll = 0\n",
    "    nent = 0\n",
    "    kl = 0\n",
    "    num_insts = 0\n",
    "    device = args.device\n",
    "    n_bits = args.n_bits\n",
    "    n_bins = args.n_bins\n",
    "    nx = args.nx\n",
    "    test_k = args.test_k\n",
    "    for data, y in val_loader:\n",
    "        batch_size = len(data)\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        loss_gen, loss_kl, loss_dequant = wolf.loss(data, y=y, n_bits=n_bits, nsamples=test_k)\n",
    "        gnll += loss_gen.sum().item()\n",
    "        kl += loss_kl.sum().item()\n",
    "        nent += loss_dequant.sum().item()\n",
    "        num_insts += batch_size\n",
    "\n",
    "    gnll = gnll / num_insts\n",
    "    nent = nent / num_insts\n",
    "    kl = kl / num_insts\n",
    "    nll = gnll + kl + nent + np.log(n_bins / 2.) * nx\n",
    "    bpd = nll / (nx * np.log(2.0))\n",
    "    nepd = nent / (nx * np.log(2.0))\n",
    "    logging('Avg  NLL: {:.2f}, KL: {:.2f}, NENT: {:.2f}, BPD: {:.4f}, NEPD: {:.4f}'.format(\n",
    "        nll, kl, nent, bpd, nepd), args.log)\n",
    "    return nll, kl, nent, bpd, nepd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_loader, train_index, train_sampler, val_loader, val_data, val_index, wolf):\n",
    "    epochs = args.epochs\n",
    "    train_k = args.train_k\n",
    "    n_bits = args.n_bits\n",
    "    n_bins = args.n_bins\n",
    "    nx = args.nx\n",
    "    grad_clip = args.grad_clip\n",
    "    batch_steps = args.batch_steps\n",
    "\n",
    "    steps_per_checkpoint = 1000\n",
    "\n",
    "    device = args.device\n",
    "    log = args.log\n",
    "\n",
    "    lr_warmups = args.warmup_steps\n",
    "    init_lr = 1e-7\n",
    "    betas = (args.beta1, args.beta2)\n",
    "    eps = args.eps\n",
    "    amsgrad = args.amsgrad\n",
    "    lr_decay = args.lr_decay\n",
    "    weight_decay = args.weight_decay\n",
    "\n",
    "    optimizer, scheduler = get_optimizer(args.lr, wolf.parameters(), betas, eps,\n",
    "                                         amsgrad=amsgrad, step_decay=lr_decay,\n",
    "                                         weight_decay=weight_decay,\n",
    "                                         warmup_steps=lr_warmups, init_lr=init_lr)\n",
    "    if args.recover >= 0:\n",
    "        checkpoint_name = args.checkpoint_name + '{}.tar'.format(args.recover)\n",
    "        print(f\"Rank = {args.rank}, loading from checkpoint {checkpoint_name}\")\n",
    "\n",
    "        checkpoint = torch.load(checkpoint_name, map_location=args.device)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        last_step = checkpoint['step']\n",
    "        wolf.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "        best_epoch = checkpoint['best_epoch']\n",
    "        best_nll = checkpoint['best_nll']\n",
    "        best_bpd = checkpoint['best_bpd']\n",
    "        best_nent = checkpoint['best_nent']\n",
    "        best_nepd = checkpoint['best_nepd']\n",
    "        best_kl = checkpoint['best_kl']\n",
    "        del checkpoint\n",
    "        if is_master(args.rank):\n",
    "            with torch.no_grad():\n",
    "                logging('Evaluating after resuming model...', log)\n",
    "                eval(args, val_loader, wolf)\n",
    "    else:\n",
    "        start_epoch = 1\n",
    "        last_step = -1\n",
    "        best_epoch = 0\n",
    "        best_nll = 1e12\n",
    "        best_bpd = 1e12\n",
    "        best_nent = 1e12\n",
    "        best_nepd = 1e12\n",
    "        best_kl = 1e12\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        wolf.train()\n",
    "        if is_distributed(args.rank):\n",
    "            train_sampler.set_epoch(epoch)\n",
    "\n",
    "        lr = scheduler.get_lr()[0]\n",
    "        start_time = time.time()\n",
    "        if is_master(args.rank):\n",
    "            logging('Epoch: %d (lr=%.6f, betas=(%.1f, %.3f), eps=%.1e, amsgrad=%s, lr decay=%.6f, clip=%.1f, l2=%.1e, train_k=%d)' % (\n",
    "            epoch, lr, betas[0], betas[1], eps, amsgrad, lr_decay, grad_clip, weight_decay, train_k), log)\n",
    "\n",
    "        gnll = torch.Tensor([0.]).to(device)\n",
    "        kl = torch.Tensor([0.]).to(device)\n",
    "        nent = torch.Tensor([0.]).to(device)\n",
    "        num_insts = torch.Tensor([0.]).to(device)\n",
    "        num_back = 0\n",
    "        num_nans = 0\n",
    "        if args.cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # data: [batch_size, n_channel, H, W]\n",
    "        # labels: [batch_size]\n",
    "        for step, (data, y) in enumerate(train_loader):\n",
    "#             print(data)\n",
    "            print(y.shape)\n",
    "            if step <= last_step:\n",
    "                continue\n",
    "            last_step = -1\n",
    "            optimizer.zero_grad()\n",
    "            batch_size = len(data)\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            data_list = [data,] if batch_steps == 1 else data.chunk(batch_steps, dim=0)\n",
    "            y_list = [y,] if batch_steps == 1 else y.chunk(batch_steps, dim=0)\n",
    "\n",
    "            gnll_batch = 0\n",
    "            kl_batch = 0\n",
    "            nent_batch = 0\n",
    "            # disable allreduce for accumulated gradient.\n",
    "            if is_distributed(args.rank):\n",
    "                wolf.disable_allreduce()\n",
    "            for data, y in zip (data_list[:-1], y_list[:-1]):\n",
    "                loss_gen, loss_kl, loss_dequant = wolf.loss(data, y=y, n_bits=n_bits, nsamples=train_k)\n",
    "                loss_gen = loss_gen.sum()\n",
    "                loss_kl = loss_kl.sum()\n",
    "                loss_dequant = loss_dequant.sum()\n",
    "                loss = (loss_gen + loss_kl + loss_dequant) / batch_size\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    gnll_batch += loss_gen.item()\n",
    "                    kl_batch += loss_kl.item()\n",
    "                    nent_batch += loss_dequant.item()\n",
    "            # enable allreduce for the last step.\n",
    "            if is_distributed(args.rank):\n",
    "                wolf.enable_allreduce()\n",
    "            data, y = data_list[-1], y_list[-1]\n",
    "            loss_gen, loss_kl, loss_dequant = wolf.loss(data, y=y, n_bits=n_bits, nsamples=train_k)\n",
    "            loss_gen = loss_gen.sum()\n",
    "            loss_kl = loss_kl.sum()\n",
    "            loss_dequant = loss_dequant.sum()\n",
    "            loss = (loss_gen + loss_kl + loss_dequant) / batch_size\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                gnll_batch += loss_gen.item()\n",
    "                kl_batch += loss_kl.item()\n",
    "                nent_batch += loss_dequant.item()\n",
    "\n",
    "            if grad_clip > 0:\n",
    "                grad_norm = clip_grad_norm_(wolf.parameters(), grad_clip)\n",
    "            else:\n",
    "                grad_norm = total_grad_norm(wolf.parameters())\n",
    "\n",
    "            if math.isnan(grad_norm):\n",
    "                num_nans += 1\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                num_insts += batch_size\n",
    "                gnll += gnll_batch\n",
    "                kl += kl_batch\n",
    "                nent += nent_batch\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            if step % args.log_interval == 0 and is_master(args.rank):\n",
    "                sys.stdout.write(\"\\b\" * num_back)\n",
    "                sys.stdout.write(\" \" * num_back)\n",
    "                sys.stdout.write(\"\\b\" * num_back)\n",
    "                nums = max(num_insts.item(), 1)\n",
    "                train_gnll = gnll.item() / nums\n",
    "                train_kl = kl.item() / nums\n",
    "                train_nent = nent.item() / nums\n",
    "                train_nll = train_gnll + train_kl + train_nent + np.log(n_bins / 2.) * nx\n",
    "                bits_per_pixel = train_nll / (nx * np.log(2.0))\n",
    "                nent_per_pixel = train_nent / (nx * np.log(2.0))\n",
    "                curr_lr = scheduler.get_lr()[0]\n",
    "                log_info = '[{}/{} ({:.0f}%) lr={:.6f}, {}] NLL: {:.2f}, BPD: {:.4f}, KL: {:.2f}, NENT: {:.2f}, NEPD: {:.4f}'.format(\n",
    "                    step * batch_size * args.world_size, len(train_index),\n",
    "                    100. * step * batch_size * args.world_size / len(train_index), curr_lr, num_nans,\n",
    "                    train_nll, bits_per_pixel, train_kl, train_nent, nent_per_pixel)\n",
    "\n",
    "                sys.stdout.write(log_info)\n",
    "                sys.stdout.flush()\n",
    "                num_back = len(log_info)\n",
    "\n",
    "            if step > 0 and step % steps_per_checkpoint == 0 and is_master(args.rank):\n",
    "                # save checkpoint\n",
    "                checkpoint_name = args.checkpoint_name + '{}.tar'.format(step)\n",
    "                torch.save({'epoch': epoch,\n",
    "                            'step': step,\n",
    "                            'model': wolf.state_dict(),\n",
    "                            'optimizer': optimizer.state_dict(),\n",
    "                            'scheduler': scheduler.state_dict(),\n",
    "                            'best_epoch': best_epoch,\n",
    "                            'best_nll': best_nll,\n",
    "                            'best_bpd': best_bpd,\n",
    "                            'best_kl': best_kl,\n",
    "                            'best_nent': best_nent,\n",
    "                            'best_nepd': best_nepd},\n",
    "                           checkpoint_name)\n",
    "\n",
    "        if is_distributed(args.rank):\n",
    "            dist.reduce(gnll, dst=0, op=dist.ReduceOp.SUM)\n",
    "            dist.reduce(kl, dst=0, op=dist.ReduceOp.SUM)\n",
    "            dist.reduce(nent, dst=0, op=dist.ReduceOp.SUM)\n",
    "            dist.reduce(num_insts, dst=0, op=dist.ReduceOp.SUM)\n",
    "\n",
    "        if is_master(args.rank):\n",
    "            sys.stdout.write(\"\\b\" * num_back)\n",
    "            sys.stdout.write(\" \" * num_back)\n",
    "            sys.stdout.write(\"\\b\" * num_back)\n",
    "            nums = num_insts.item()\n",
    "            train_gnll = gnll.item() / nums\n",
    "            train_kl = kl.item() / nums\n",
    "            train_nent = nent.item() / nums\n",
    "            train_nll = train_gnll + train_kl + train_nent + np.log(n_bins / 2.) * nx\n",
    "            bits_per_pixel = train_nll / (nx * np.log(2.0))\n",
    "            nent_per_pixel = train_nent / (nx * np.log(2.0))\n",
    "            logging('Average NLL: {:.2f}, BPD: {:.4f}, KL: {:.2f}, NENT: {:.2f}, NEPD: {:.4f}, time: {:.1f}s'.format(\n",
    "                    train_nll, bits_per_pixel, train_kl, train_nent, nent_per_pixel, time.time() - start_time), log)\n",
    "            logging('-' * 125, log)\n",
    "\n",
    "            if epoch < args.valid_epochs or epoch % args.valid_epochs == 0:\n",
    "                with torch.no_grad():\n",
    "                    nll, kl, nent, bpd, nepd = eval(args, val_loader, wolf)\n",
    "                    if nll < best_nll:\n",
    "                        best_epoch = epoch\n",
    "                        best_nll = nll\n",
    "                        best_bpd = bpd\n",
    "                        best_kl = kl\n",
    "                        best_nent = nent\n",
    "                        best_nepd = nepd\n",
    "                        wolf.save(args.model_path)\n",
    "                        checkpoint_name = args.checkpoint_name + '{}.tar'.format(0)\n",
    "                        torch.save({'epoch': epoch + 1,\n",
    "                                    'step': -1,\n",
    "                                    'model': wolf.state_dict(),\n",
    "                                    'optimizer': optimizer.state_dict(),\n",
    "                                    'scheduler': scheduler.state_dict(),\n",
    "                                    'best_epoch': best_epoch,\n",
    "                                    'best_nll': best_nll,\n",
    "                                    'best_bpd': best_bpd,\n",
    "                                    'best_kl': best_kl,\n",
    "                                    'best_nent': best_nent,\n",
    "                                    'best_nepd': best_nepd},\n",
    "                                   checkpoint_name)\n",
    "                    try:\n",
    "                        reconstruct(args, epoch, val_data, val_index, wolf)\n",
    "                    except RuntimeError:\n",
    "                        print('Reconstruction failed.')\n",
    "                    try:\n",
    "                        sample(args, epoch, wolf)\n",
    "                    except RuntimeError:\n",
    "                        print('Sampling failed')\n",
    "            logging('Best NLL: {:.2f}, KL: {:.2f}, NENT: {:.2f}, BPD: {:.4f}, NEPD: {:.4f}, epoch: {}'.format(\n",
    "                best_nll, best_kl, best_nent, best_bpd, best_nepd, best_epoch), log)\n",
    "            logging('=' * 125, log)\n",
    "            # save checkpoint\n",
    "            checkpoint_name = args.checkpoint_name + '{}.tar'.format(1)\n",
    "            torch.save({'epoch': epoch + 1,\n",
    "                        'step': -1,\n",
    "                        'model': wolf.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict(),\n",
    "                        'best_epoch': best_epoch,\n",
    "                        'best_nll': best_nll,\n",
    "                        'best_bpd': best_bpd,\n",
    "                        'best_kl': best_kl,\n",
    "                        'best_nent': best_nent,\n",
    "                        'best_nepd': best_nepd},\n",
    "                       checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    args, (train_data, val_data), (train_index, val_index), wolf = setup(args)\n",
    "    \n",
    "    if is_master(args.rank):\n",
    "        logging('# of Parameters: %d' % sum([param.numel() for param in wolf.parameters()]), args.log)\n",
    "        if args.recover < 0:\n",
    "            init_model(args, train_data, train_index, wolf)\n",
    "            wolf.sync()\n",
    "\n",
    "    if is_distributed(args.rank):\n",
    "        wolf.init_distributed(args.rank, args.local_rank)\n",
    "\n",
    "    train_loader, train_sampler, val_loader = init_dataloader(args, train_data, val_data)\n",
    "\n",
    "    train(args, train_loader, train_index, train_sampler, val_loader, val_data, val_index, wolf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args_dict = {'rank': -1,\n",
    "#  'local_rank': 0,\n",
    "#  'config': 'experiments/configs/cifar10/glow/glow-cat-uni.json',\n",
    "#  'batch_size': 256,\n",
    "#  'eval_batch_size': 1000,\n",
    "#  'batch_steps': 2,\n",
    "#  'init_batch_size': 1024,\n",
    "#  'epochs': 100,\n",
    "#  'valid_epochs': 10,\n",
    "#  'seed': 65537,\n",
    "#  'train_k': 1,\n",
    "#  'log_interval': 10,\n",
    "#  'lr': 0.001,\n",
    "#  'warmup_steps': 50,\n",
    "#  'lr_decay': 0.999997,\n",
    "#  'beta1': 0.9,\n",
    "#  'beta2': 0.999,\n",
    "#  'eps': 1e-08,\n",
    "#  'weight_decay': 1e-06,\n",
    "#  'amsgrad': False,\n",
    "#  'grad_clip': 0.0,\n",
    "#  'dataset': 'cifar10',\n",
    "#  'category': None,\n",
    "#  'image_size': 32,\n",
    "#  'workers': 4,\n",
    "#  'n_bits': 8,\n",
    "#  'model_path': 'experiments/models/save_model',\n",
    "#  'data_path': 'experiments/data/cifar_data1',\n",
    "#  'recover': -1}\n",
    "\n",
    "args_dict = {'rank': -1,\n",
    " 'local_rank': 0,\n",
    " 'config': 'experiments/configs/celebA-HQ/glow/glow-base-uni.json',\n",
    " 'batch_size': 10,\n",
    " 'eval_batch_size': 10,\n",
    " 'batch_steps': 2,\n",
    " 'init_batch_size': 10,\n",
    " 'epochs': 100,\n",
    " 'valid_epochs': 10,\n",
    " 'seed': 65537,\n",
    " 'train_k': 1,\n",
    " 'log_interval': 10,\n",
    " 'lr': 0.001,\n",
    " 'warmup_steps': 200,\n",
    " 'lr_decay': 0.999997,\n",
    " 'beta1': 0.9,\n",
    " 'beta2': 0.999,\n",
    " 'eps': 1e-08,\n",
    " 'weight_decay': 5e-04,\n",
    " 'amsgrad': False,\n",
    " 'grad_clip': 0.0,\n",
    " 'dataset': 'celeba',\n",
    " 'category': None,\n",
    " 'image_size': 256,\n",
    " 'workers': 4,\n",
    " 'n_bits': 8,\n",
    " 'model_path': 'experiments/models/celeba_model',\n",
    " 'data_path': 'experiments/data/celeba_data',\n",
    " 'recover': -1}\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank -1: random seed=65537\n",
      "Rank -1: Namespace(amsgrad=False, batch_size=10, batch_steps=2, beta1=0.9, beta2=0.999, category=None, checkpoint_name='experiments/models/celeba_model/checkpoint', config='experiments/configs/celebA-HQ/glow/glow-base-uni.json', cuda=True, data_path='experiments/data/celeba_data', dataset='celeba', epochs=100, eps=1e-08, eval_batch_size=10, grad_clip=0.0, image_size=256, init_batch_size=10, local_rank=0, log=<_io.TextIOWrapper name='experiments/models/celeba_model/log.txt' mode='w' encoding='UTF-8'>, log_interval=10, lr=0.001, lr_decay=0.999997, model_path='experiments/models/celeba_model', n_bins=256.0, n_bits=8, nx=196608, rank=-1, recover=-1, result_path='experiments/models/celeba_model/images', seed=65537, test_k=5, train_k=1, valid_epochs=10, warmup_steps=200, weight_decay=0.0005, workers=4, world_size=1)\n",
      "Celeb-A has already been extracted.\n",
      "The number of extraction is 202599\n",
      "Data size: training: 162079, val: 40520\n",
      "# of Parameters: 70714159\n",
      "Rank -1, init model: 10 instances\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 40])\n",
      "Epoch: 1 (lr=0.000000, betas=(0.9, 0.999), eps=1.0e-08, amsgrad=False, lr decay=0.999997, clip=0.0, l2=5.0e-04, train_k=1)\n",
      "torch.Size([10, 40])\n"
     ]
    }
   ],
   "source": [
    "assert args.rank == -1 and args.local_rank == 0, 'single process should have wrong rank ({}) or local rank ({})'.format(args.rank, args.local_rank)\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wolf",
   "language": "python",
   "name": "wolf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
